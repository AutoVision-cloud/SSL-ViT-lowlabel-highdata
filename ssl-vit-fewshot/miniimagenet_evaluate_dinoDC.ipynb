{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Reference: https://github.com/ShuoYang-1998/Few_Shot_Distribution_Calibration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================== Imports ====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "use_gpu = torch.cuda.is_available()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============ Distribution Calibration Code =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def distribution_calibration(query, base_means, base_cov, k,alpha=0.21):\n",
    "    dist = []\n",
    "    for i in range(len(base_means)):\n",
    "        dist.append(np.linalg.norm(query-base_means[i]))\n",
    "    index = np.argpartition(dist, k)[:k]\n",
    "    #print(index)\n",
    "    mean = np.concatenate([np.array(base_means)[index], query[np.newaxis, :]])\n",
    "    calibrated_mean = np.mean(mean, axis=0)\n",
    "    calibrated_cov = np.mean(np.array(base_cov)[index], axis=0)+alpha\n",
    "    return calibrated_mean, calibrated_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Setup data, nway-nshot task =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# ---- data loading\n",
    "dataset = 'miniImagenet'\n",
    "n_shot = 1 # 5\n",
    "n_ways = 5\n",
    "n_queries = 15\n",
    "n_runs = 10000\n",
    "n_lsamples = n_ways * n_shot\n",
    "n_usamples = n_ways * n_queries\n",
    "n_samples = n_lsamples + n_usamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "#   Usefull paths\n",
    "_datasetFeaturesFiles = {\"miniImagenet\": \"./dino_features_data/dino_test_miniimagenet.p\",}\n",
    "_cacheDir = \"cache\"\n",
    "_maxRuns = 10000\n",
    "_min_examples = -1.\n",
    "\n",
    "# ========================================================\n",
    "#   Module internal functions and variables\n",
    "\n",
    "_randStates = None\n",
    "_rsCfg = None\n",
    "\n",
    "\n",
    "def _load_pickle(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        labels = [np.full(shape=len(data[key]), fill_value=key)\n",
    "                  for key in data]\n",
    "        data = [features for key in data for features in data[key]]\n",
    "        dataset = dict()\n",
    "        dataset['data'] = torch.FloatTensor(np.stack(data, axis=0))\n",
    "        dataset['labels'] = torch.LongTensor(np.concatenate(labels))\n",
    "        return dataset\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "#    Callable variables and functions from outside the module\n",
    "\n",
    "data = None\n",
    "labels = None\n",
    "dsName = None\n",
    "\n",
    "\n",
    "def loadDataSet(dsname):\n",
    "    if dsname not in _datasetFeaturesFiles:\n",
    "        raise NameError('Unknwown dataset: {}'.format(dsname))\n",
    "\n",
    "    global dsName, data, labels, _randStates, _rsCfg, _min_examples\n",
    "    dsName = dsname\n",
    "    _randStates = None\n",
    "    _rsCfg = None\n",
    "\n",
    "    # Loading data from files on computer\n",
    "    # home = expanduser(\"~\")\n",
    "    dataset = _load_pickle(_datasetFeaturesFiles[dsname])\n",
    "\n",
    "    # Computing the number of items per class in the dataset\n",
    "    _min_examples = dataset[\"labels\"].shape[0]\n",
    "    for i in range(dataset[\"labels\"].shape[0]):\n",
    "        if torch.where(dataset[\"labels\"] == dataset[\"labels\"][i])[0].shape[0] > 0:\n",
    "            _min_examples = min(_min_examples, torch.where(\n",
    "                dataset[\"labels\"] == dataset[\"labels\"][i])[0].shape[0])\n",
    "    print(\"Guaranteed number of items per class: {:d}\\n\".format(_min_examples))\n",
    "\n",
    "    # Generating data tensors\n",
    "    data = torch.zeros((0, _min_examples, dataset[\"data\"].shape[1]))\n",
    "    labels = dataset[\"labels\"].clone()\n",
    "    while labels.shape[0] > 0:\n",
    "        indices = torch.where(dataset[\"labels\"] == labels[0])[0]\n",
    "        data = torch.cat([data, dataset[\"data\"][indices, :]\n",
    "                          [:_min_examples].view(1, _min_examples, -1)], dim=0)\n",
    "        indices = torch.where(labels != labels[0])[0]\n",
    "        labels = labels[indices]\n",
    "    print(\"Total of {:d} classes, {:d} elements each, with dimension {:d}\\n\".format(\n",
    "        data.shape[0], data.shape[1], data.shape[2]))\n",
    "\n",
    "\n",
    "def GenerateRun(iRun, cfg, regenRState=False, generate=True):\n",
    "    global _randStates, data, _min_examples\n",
    "    if not regenRState:\n",
    "        np.random.set_state(_randStates[iRun])\n",
    "\n",
    "    classes = np.random.permutation(np.arange(data.shape[0]))[:cfg[\"ways\"]]\n",
    "    shuffle_indices = np.arange(_min_examples)\n",
    "    dataset = None\n",
    "    actual_labels = None\n",
    "    if generate:\n",
    "        dataset = torch.zeros((cfg['ways'], cfg['shot']+cfg['queries'], data.shape[2]))\n",
    "        actual_labels = torch.zeros((cfg['ways']))\n",
    "    for i in range(cfg['ways']):\n",
    "        shuffle_indices = np.random.permutation(shuffle_indices)\n",
    "        if generate:\n",
    "            dataset[i] = data[classes[i], shuffle_indices,:][:cfg['shot']+cfg['queries']]\n",
    "            actual_labels[i] = classes[i]\n",
    "    return dataset, actual_labels\n",
    "\n",
    "\n",
    "def ClassesInRun(iRun, cfg):\n",
    "    global _randStates, data\n",
    "    np.random.set_state(_randStates[iRun])\n",
    "\n",
    "    classes = np.random.permutation(np.arange(data.shape[0]))[:cfg[\"ways\"]]\n",
    "    return classes\n",
    "\n",
    "\n",
    "def setRandomStates(cfg):\n",
    "    global _randStates, _maxRuns, _rsCfg\n",
    "    if _rsCfg == cfg:\n",
    "        return\n",
    "\n",
    "    rsFile = os.path.join(_cacheDir, \"RandStates_{}_s{}_q{}_w{}\".format(\n",
    "        dsName, cfg['shot'], cfg['queries'], cfg['ways']))\n",
    "    if not os.path.exists(rsFile):\n",
    "        print(\"{} does not exist, regenerating it...\".format(rsFile))\n",
    "        np.random.seed(0)\n",
    "        _randStates = []\n",
    "        for iRun in range(_maxRuns):\n",
    "            _randStates.append(np.random.get_state())\n",
    "            GenerateRun(iRun, cfg, regenRState=True, generate=False)\n",
    "        torch.save(_randStates, rsFile)\n",
    "    else:\n",
    "        print(\"reloading random states from file....\")\n",
    "        _randStates = torch.load(rsFile)\n",
    "    _rsCfg = cfg\n",
    "\n",
    "\n",
    "def GenerateRunSet(start=None, end=None, cfg=None):\n",
    "    global dataset, _maxRuns, actual_classes\n",
    "    if start is None:\n",
    "        start = 0\n",
    "    if end is None:\n",
    "        end = _maxRuns\n",
    "    if cfg is None:\n",
    "        cfg = {\"shot\": 1, \"ways\": 5, \"queries\": 15}\n",
    "\n",
    "    setRandomStates(cfg)\n",
    "    print(\"generating task from {} to {}\".format(start, end))\n",
    "\n",
    "    dataset = torch.zeros((end-start, cfg['ways'], cfg['shot']+cfg['queries'], data.shape[2]))\n",
    "    actual_classes = torch.zeros((end-start, cfg['ways']))\n",
    "    \n",
    "    for iRun in range(end-start):\n",
    "        dataset[iRun], actual_classes[iRun] = GenerateRun(start+iRun, cfg)\n",
    "\n",
    "    return dataset, actual_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import FSLTask\n",
    "cfg = {'shot': n_shot, 'ways': n_ways, 'queries': n_queries}\n",
    "\n",
    "loadDataSet(dataset)\n",
    "\n",
    "setRandomStates(cfg)\n",
    "\n",
    "ndatas, actual_classes = GenerateRunSet(end=n_runs, cfg=cfg)\n",
    "ndatas = ndatas.permute(0, 2, 1, 3).reshape(n_runs, n_samples, -1)\n",
    "labels = torch.arange(n_ways).view(1, 1, n_ways).expand(n_runs, n_shot + n_queries, n_ways).clone().view(n_runs, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===== Get the base-class stats for Distribution Calibration ======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# ---- Base class statistics\n",
    "base_means = []\n",
    "base_cov = [] \n",
    "dataset = 'miniImagenet'\n",
    "#base_features_path = \"../checkpoints/%s/WideResNet28_10_S2M2_R/last/base_features.plk\"%dataset\n",
    "base_features_path = \"./dino_features_data/dino_train_miniimagenet.p\"\n",
    "with open(base_features_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    for key in data.keys():\n",
    "        feature = np.array(data[key])\n",
    "        mean = np.mean(feature, axis=0)\n",
    "        cov = np.cov(feature.T)\n",
    "        base_means.append(mean)\n",
    "        base_cov.append(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Logging =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='[{%(filename)s:%(lineno)d} %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(filename='logs/experiment_errors.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('LOGGER_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== DC + Logistic Regression =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- classification for each task\n",
    "acc_list = []\n",
    "\n",
    "print('Start classification for %d tasks...'%(n_runs))\n",
    "\n",
    "keep_count = 0\n",
    "\n",
    "for i in tqdm(range(n_runs)):\n",
    "    keep_count = i\n",
    "    support_data = ndatas[i][:n_lsamples].numpy()\n",
    "    support_label = labels[i][:n_lsamples].numpy()\n",
    "    query_data = ndatas[i][n_lsamples:].numpy()\n",
    "    query_label = labels[i][n_lsamples:].numpy()\n",
    "    \n",
    "    # ---- Tukey's transform\n",
    "    # beta = 0.5\n",
    "    #support_data = np.power(support_data[:, ] ,beta)\n",
    "    #query_data = np.power(query_data[:, ] ,beta)\n",
    "    \n",
    "    # ---- distribution calibration and feature sampling\n",
    "    sampled_data = []\n",
    "    sampled_label = []\n",
    "    num_sampled = int(750/n_shot)\n",
    "    \n",
    "    for i in range(n_lsamples):\n",
    "        mean, cov = distribution_calibration(support_data[i], base_means, base_cov, k=2)\n",
    "        sampled_data.append(np.random.multivariate_normal(mean=mean, cov=cov, size=num_sampled))\n",
    "        sampled_label.extend([support_label[i]]*num_sampled)\n",
    "\n",
    "    sampled_data = np.concatenate([sampled_data[:]]).reshape(n_ways * n_shot * num_sampled, -1)\n",
    "    \n",
    "    X_aug = np.concatenate([support_data, sampled_data])\n",
    "    Y_aug = np.concatenate([support_label, sampled_label])\n",
    "    \n",
    "    # ---- train classifier\n",
    "    classifier = LogisticRegression(max_iter=1000).fit(X=X_aug, y=Y_aug)\n",
    "    #classifier = KNeighborsClassifier(n_neighbors=3).fit(X=X_aug, y=Y_aug)\n",
    "    predicts = classifier.predict(query_data)\n",
    "    \n",
    "    \n",
    "    acc = np.mean(predicts == query_label)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "    if keep_count % 100 == 0:\n",
    "        logger.info('Iteration  %d %f %f' %(keep_count, acc, np.mean(acc_list)))\n",
    "\n",
    "logger.info('Final: Mini-Imagenet') \n",
    "logger.info('%s %d way %d shot  ACC : %f'%(dataset,n_ways,n_shot,float(np.mean(acc_list))))\n",
    "logger.info('Time needed: %f' %(time.time()-start_time))\n",
    "print('%s %d way %d shot  ACC : %f'%(dataset,n_ways,n_shot,float(np.mean(acc_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3Ddetector",
   "language": "python",
   "name": "3ddetector"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
